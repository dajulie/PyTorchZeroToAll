{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lec_9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOCsfDNVR4N3MWRGIcM4HaA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dajulie/PyTorchZeroToAll/blob/main/lec_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JE6d2tg-UNzK",
        "outputId": "ca2332a0-b556-44a6-82cc-7bf00dba2f59"
      },
      "source": [
        "from torch import nn, tensor, max\n",
        "import numpy as np\n",
        "\n",
        "Y = np.array([1, 0, 0])\n",
        "Y_pred1 = np.array([0.7, 0.2, 0.1])\n",
        "Y_pred2 = np.array([0.1, 0.3, 0.6])\n",
        "print(f'Loss1: {np.sum(-Y * np.log(Y_pred1)):.4f}')\n",
        "print(f'Loss2: {np.sum(-Y * np.log(Y_pred2)):.4f}')\n",
        "\n",
        "\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "Y = tensor([0], requires_grad=False)\n",
        "\n",
        "Y_pred1 = tensor([[2.0, 1.0, 0.1]])\n",
        "Y_pred2 = tensor([[0.5, 2.0, 0.3]])\n",
        "\n",
        "l1 = loss(Y_pred1, Y)\n",
        "l2 = loss(Y_pred2, Y)\n",
        "\n",
        "print(f'PyTorch Loss1: {l1.item():.4f} \\nPyTorch Loss2: {l2.item():.4f}')\n",
        "print(f'Y_pred1: {max(Y_pred1.data, 1)[1].item()}')\n",
        "print(f'Y_pred2: {max(Y_pred2.data, 1)[1].item()}')\n",
        "\n",
        "\n",
        "\n",
        "Y = tensor([2, 0, 1], requires_grad=False)\n",
        "\n",
        "Y_pred1 = tensor([[0.1, 0.2, 0.9],\n",
        "                  [1.1, 0.1, 0.2],\n",
        "                  [0.2, 2.1, 0.1]])\n",
        "\n",
        "Y_pred2 = tensor([[0.8, 0.2, 0.3],\n",
        "                  [0.2, 0.3, 0.5],\n",
        "                  [0.2, 0.2, 0.5]])\n",
        "\n",
        "l1 = loss(Y_pred1, Y)\n",
        "l2 = loss(Y_pred2, Y)\n",
        "print(f'Batch Loss1: {l1.item():.4f} \\nBatch Loss2: {l2.data:.4f}')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss1: 0.3567\n",
            "Loss2: 2.3026\n",
            "PyTorch Loss1: 0.4170 \n",
            "PyTorch Loss2: 1.8406\n",
            "Y_pred1: 0\n",
            "Y_pred2: 1\n",
            "Batch Loss1: 0.4966 \n",
            "Batch Loss2: 1.2389\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2j22JIFYLBl",
        "outputId": "f949bdd3-9068-47d4-8d51-84ed430cf147"
      },
      "source": [
        "from __future__ import print_function\n",
        "from torch import nn, optim, cuda\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "batch_size = 64\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(f'Training MNIST Model on {device}\\n{\"=\" * 44}')\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.l1 = nn.Linear(784, 520)\n",
        "        self.l2 = nn.Linear(520, 320)\n",
        "        self.l3 = nn.Linear(320, 240)\n",
        "        self.l4 = nn.Linear(240, 120)\n",
        "        self.l5 = nn.Linear(120, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.l1(x))\n",
        "        x = F.relu(self.l2(x))\n",
        "        x = F.relu(self.l3(x))\n",
        "        x = F.relu(self.l4(x))\n",
        "        return self.l5(x)\n",
        "\n",
        "\n",
        "model = Net()\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        test_loss += criterion(output, target).item()\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'===========================\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
        "          f'({100. * correct / len(test_loader.dataset):.0f}%)')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    since = time.time()\n",
        "    for epoch in range(1, 5):\n",
        "        epoch_start = time.time()\n",
        "        train(epoch)\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
        "        test()\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
        "\n",
        "    m, s = divmod(time.time() - since, 60)\n",
        "    print(f'Total Time: {m:.0f}m {s:.0f}s\\nModel was trained on {device}!')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training MNIST Model on cpu\n",
            "============================================\n",
            "Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 2.303748\n",
            "Train Epoch: 1 | Batch Status: 640/60000 (1%) | Loss: 2.299030\n",
            "Train Epoch: 1 | Batch Status: 1280/60000 (2%) | Loss: 2.298820\n",
            "Train Epoch: 1 | Batch Status: 1920/60000 (3%) | Loss: 2.308497\n",
            "Train Epoch: 1 | Batch Status: 2560/60000 (4%) | Loss: 2.305350\n",
            "Train Epoch: 1 | Batch Status: 3200/60000 (5%) | Loss: 2.287960\n",
            "Train Epoch: 1 | Batch Status: 3840/60000 (6%) | Loss: 2.305628\n",
            "Train Epoch: 1 | Batch Status: 4480/60000 (7%) | Loss: 2.302193\n",
            "Train Epoch: 1 | Batch Status: 5120/60000 (9%) | Loss: 2.294440\n",
            "Train Epoch: 1 | Batch Status: 5760/60000 (10%) | Loss: 2.296069\n",
            "Train Epoch: 1 | Batch Status: 6400/60000 (11%) | Loss: 2.304749\n",
            "Train Epoch: 1 | Batch Status: 7040/60000 (12%) | Loss: 2.294084\n",
            "Train Epoch: 1 | Batch Status: 7680/60000 (13%) | Loss: 2.305148\n",
            "Train Epoch: 1 | Batch Status: 8320/60000 (14%) | Loss: 2.292041\n",
            "Train Epoch: 1 | Batch Status: 8960/60000 (15%) | Loss: 2.294998\n",
            "Train Epoch: 1 | Batch Status: 9600/60000 (16%) | Loss: 2.294662\n",
            "Train Epoch: 1 | Batch Status: 10240/60000 (17%) | Loss: 2.293360\n",
            "Train Epoch: 1 | Batch Status: 10880/60000 (18%) | Loss: 2.299731\n",
            "Train Epoch: 1 | Batch Status: 11520/60000 (19%) | Loss: 2.294063\n",
            "Train Epoch: 1 | Batch Status: 12160/60000 (20%) | Loss: 2.296439\n",
            "Train Epoch: 1 | Batch Status: 12800/60000 (21%) | Loss: 2.292168\n",
            "Train Epoch: 1 | Batch Status: 13440/60000 (22%) | Loss: 2.289597\n",
            "Train Epoch: 1 | Batch Status: 14080/60000 (23%) | Loss: 2.298060\n",
            "Train Epoch: 1 | Batch Status: 14720/60000 (25%) | Loss: 2.294128\n",
            "Train Epoch: 1 | Batch Status: 15360/60000 (26%) | Loss: 2.298723\n",
            "Train Epoch: 1 | Batch Status: 16000/60000 (27%) | Loss: 2.285913\n",
            "Train Epoch: 1 | Batch Status: 16640/60000 (28%) | Loss: 2.291030\n",
            "Train Epoch: 1 | Batch Status: 17280/60000 (29%) | Loss: 2.287642\n",
            "Train Epoch: 1 | Batch Status: 17920/60000 (30%) | Loss: 2.286311\n",
            "Train Epoch: 1 | Batch Status: 18560/60000 (31%) | Loss: 2.284656\n",
            "Train Epoch: 1 | Batch Status: 19200/60000 (32%) | Loss: 2.284458\n",
            "Train Epoch: 1 | Batch Status: 19840/60000 (33%) | Loss: 2.283613\n",
            "Train Epoch: 1 | Batch Status: 20480/60000 (34%) | Loss: 2.294269\n",
            "Train Epoch: 1 | Batch Status: 21120/60000 (35%) | Loss: 2.283816\n",
            "Train Epoch: 1 | Batch Status: 21760/60000 (36%) | Loss: 2.285011\n",
            "Train Epoch: 1 | Batch Status: 22400/60000 (37%) | Loss: 2.276375\n",
            "Train Epoch: 1 | Batch Status: 23040/60000 (38%) | Loss: 2.284154\n",
            "Train Epoch: 1 | Batch Status: 23680/60000 (39%) | Loss: 2.273805\n",
            "Train Epoch: 1 | Batch Status: 24320/60000 (41%) | Loss: 2.272095\n",
            "Train Epoch: 1 | Batch Status: 24960/60000 (42%) | Loss: 2.273184\n",
            "Train Epoch: 1 | Batch Status: 25600/60000 (43%) | Loss: 2.268141\n",
            "Train Epoch: 1 | Batch Status: 26240/60000 (44%) | Loss: 2.281865\n",
            "Train Epoch: 1 | Batch Status: 26880/60000 (45%) | Loss: 2.275704\n",
            "Train Epoch: 1 | Batch Status: 27520/60000 (46%) | Loss: 2.271003\n",
            "Train Epoch: 1 | Batch Status: 28160/60000 (47%) | Loss: 2.263203\n",
            "Train Epoch: 1 | Batch Status: 28800/60000 (48%) | Loss: 2.260214\n",
            "Train Epoch: 1 | Batch Status: 29440/60000 (49%) | Loss: 2.273629\n",
            "Train Epoch: 1 | Batch Status: 30080/60000 (50%) | Loss: 2.257305\n",
            "Train Epoch: 1 | Batch Status: 30720/60000 (51%) | Loss: 2.276874\n",
            "Train Epoch: 1 | Batch Status: 31360/60000 (52%) | Loss: 2.265368\n",
            "Train Epoch: 1 | Batch Status: 32000/60000 (53%) | Loss: 2.251146\n",
            "Train Epoch: 1 | Batch Status: 32640/60000 (54%) | Loss: 2.253886\n",
            "Train Epoch: 1 | Batch Status: 33280/60000 (55%) | Loss: 2.267282\n",
            "Train Epoch: 1 | Batch Status: 33920/60000 (57%) | Loss: 2.252772\n",
            "Train Epoch: 1 | Batch Status: 34560/60000 (58%) | Loss: 2.255857\n",
            "Train Epoch: 1 | Batch Status: 35200/60000 (59%) | Loss: 2.235863\n",
            "Train Epoch: 1 | Batch Status: 35840/60000 (60%) | Loss: 2.251087\n",
            "Train Epoch: 1 | Batch Status: 36480/60000 (61%) | Loss: 2.243835\n",
            "Train Epoch: 1 | Batch Status: 37120/60000 (62%) | Loss: 2.235575\n",
            "Train Epoch: 1 | Batch Status: 37760/60000 (63%) | Loss: 2.212173\n",
            "Train Epoch: 1 | Batch Status: 38400/60000 (64%) | Loss: 2.224651\n",
            "Train Epoch: 1 | Batch Status: 39040/60000 (65%) | Loss: 2.214941\n",
            "Train Epoch: 1 | Batch Status: 39680/60000 (66%) | Loss: 2.209249\n",
            "Train Epoch: 1 | Batch Status: 40320/60000 (67%) | Loss: 2.196469\n",
            "Train Epoch: 1 | Batch Status: 40960/60000 (68%) | Loss: 2.204637\n",
            "Train Epoch: 1 | Batch Status: 41600/60000 (69%) | Loss: 2.173728\n",
            "Train Epoch: 1 | Batch Status: 42240/60000 (70%) | Loss: 2.172538\n",
            "Train Epoch: 1 | Batch Status: 42880/60000 (71%) | Loss: 2.177266\n",
            "Train Epoch: 1 | Batch Status: 43520/60000 (72%) | Loss: 2.127104\n",
            "Train Epoch: 1 | Batch Status: 44160/60000 (74%) | Loss: 2.131451\n",
            "Train Epoch: 1 | Batch Status: 44800/60000 (75%) | Loss: 2.084965\n",
            "Train Epoch: 1 | Batch Status: 45440/60000 (76%) | Loss: 2.123293\n",
            "Train Epoch: 1 | Batch Status: 46080/60000 (77%) | Loss: 2.095023\n",
            "Train Epoch: 1 | Batch Status: 46720/60000 (78%) | Loss: 2.084336\n",
            "Train Epoch: 1 | Batch Status: 47360/60000 (79%) | Loss: 2.056655\n",
            "Train Epoch: 1 | Batch Status: 48000/60000 (80%) | Loss: 1.973844\n",
            "Train Epoch: 1 | Batch Status: 48640/60000 (81%) | Loss: 1.943603\n",
            "Train Epoch: 1 | Batch Status: 49280/60000 (82%) | Loss: 1.876732\n",
            "Train Epoch: 1 | Batch Status: 49920/60000 (83%) | Loss: 1.879513\n",
            "Train Epoch: 1 | Batch Status: 50560/60000 (84%) | Loss: 1.797046\n",
            "Train Epoch: 1 | Batch Status: 51200/60000 (85%) | Loss: 1.810509\n",
            "Train Epoch: 1 | Batch Status: 51840/60000 (86%) | Loss: 1.713312\n",
            "Train Epoch: 1 | Batch Status: 52480/60000 (87%) | Loss: 1.706274\n",
            "Train Epoch: 1 | Batch Status: 53120/60000 (88%) | Loss: 1.735558\n",
            "Train Epoch: 1 | Batch Status: 53760/60000 (90%) | Loss: 1.606517\n",
            "Train Epoch: 1 | Batch Status: 54400/60000 (91%) | Loss: 1.492285\n",
            "Train Epoch: 1 | Batch Status: 55040/60000 (92%) | Loss: 1.468133\n",
            "Train Epoch: 1 | Batch Status: 55680/60000 (93%) | Loss: 1.438211\n",
            "Train Epoch: 1 | Batch Status: 56320/60000 (94%) | Loss: 1.474313\n",
            "Train Epoch: 1 | Batch Status: 56960/60000 (95%) | Loss: 1.306396\n",
            "Train Epoch: 1 | Batch Status: 57600/60000 (96%) | Loss: 1.344096\n",
            "Train Epoch: 1 | Batch Status: 58240/60000 (97%) | Loss: 1.074371\n",
            "Train Epoch: 1 | Batch Status: 58880/60000 (98%) | Loss: 0.881423\n",
            "Train Epoch: 1 | Batch Status: 59520/60000 (99%) | Loss: 1.161787\n",
            "Training time: 0m 12s\n",
            "===========================\n",
            "Test set: Average loss: 0.0158, Accuracy: 7065/10000 (71%)\n",
            "Testing time: 0m 14s\n",
            "Train Epoch: 2 | Batch Status: 0/60000 (0%) | Loss: 1.046871\n",
            "Train Epoch: 2 | Batch Status: 640/60000 (1%) | Loss: 0.858102\n",
            "Train Epoch: 2 | Batch Status: 1280/60000 (2%) | Loss: 1.035728\n",
            "Train Epoch: 2 | Batch Status: 1920/60000 (3%) | Loss: 1.070191\n",
            "Train Epoch: 2 | Batch Status: 2560/60000 (4%) | Loss: 0.876545\n",
            "Train Epoch: 2 | Batch Status: 3200/60000 (5%) | Loss: 0.912126\n",
            "Train Epoch: 2 | Batch Status: 3840/60000 (6%) | Loss: 0.785901\n",
            "Train Epoch: 2 | Batch Status: 4480/60000 (7%) | Loss: 0.883668\n",
            "Train Epoch: 2 | Batch Status: 5120/60000 (9%) | Loss: 0.799706\n",
            "Train Epoch: 2 | Batch Status: 5760/60000 (10%) | Loss: 0.773507\n",
            "Train Epoch: 2 | Batch Status: 6400/60000 (11%) | Loss: 0.704533\n",
            "Train Epoch: 2 | Batch Status: 7040/60000 (12%) | Loss: 0.884840\n",
            "Train Epoch: 2 | Batch Status: 7680/60000 (13%) | Loss: 0.658192\n",
            "Train Epoch: 2 | Batch Status: 8320/60000 (14%) | Loss: 0.668504\n",
            "Train Epoch: 2 | Batch Status: 8960/60000 (15%) | Loss: 0.948244\n",
            "Train Epoch: 2 | Batch Status: 9600/60000 (16%) | Loss: 0.599106\n",
            "Train Epoch: 2 | Batch Status: 10240/60000 (17%) | Loss: 0.638521\n",
            "Train Epoch: 2 | Batch Status: 10880/60000 (18%) | Loss: 0.913314\n",
            "Train Epoch: 2 | Batch Status: 11520/60000 (19%) | Loss: 0.703700\n",
            "Train Epoch: 2 | Batch Status: 12160/60000 (20%) | Loss: 0.555711\n",
            "Train Epoch: 2 | Batch Status: 12800/60000 (21%) | Loss: 0.639295\n",
            "Train Epoch: 2 | Batch Status: 13440/60000 (22%) | Loss: 0.696110\n",
            "Train Epoch: 2 | Batch Status: 14080/60000 (23%) | Loss: 0.744873\n",
            "Train Epoch: 2 | Batch Status: 14720/60000 (25%) | Loss: 0.649114\n",
            "Train Epoch: 2 | Batch Status: 15360/60000 (26%) | Loss: 0.514675\n",
            "Train Epoch: 2 | Batch Status: 16000/60000 (27%) | Loss: 0.799552\n",
            "Train Epoch: 2 | Batch Status: 16640/60000 (28%) | Loss: 0.539498\n",
            "Train Epoch: 2 | Batch Status: 17280/60000 (29%) | Loss: 0.301188\n",
            "Train Epoch: 2 | Batch Status: 17920/60000 (30%) | Loss: 0.574248\n",
            "Train Epoch: 2 | Batch Status: 18560/60000 (31%) | Loss: 0.472237\n",
            "Train Epoch: 2 | Batch Status: 19200/60000 (32%) | Loss: 0.425238\n",
            "Train Epoch: 2 | Batch Status: 19840/60000 (33%) | Loss: 0.538055\n",
            "Train Epoch: 2 | Batch Status: 20480/60000 (34%) | Loss: 0.897900\n",
            "Train Epoch: 2 | Batch Status: 21120/60000 (35%) | Loss: 0.577872\n",
            "Train Epoch: 2 | Batch Status: 21760/60000 (36%) | Loss: 0.399661\n",
            "Train Epoch: 2 | Batch Status: 22400/60000 (37%) | Loss: 0.633588\n",
            "Train Epoch: 2 | Batch Status: 23040/60000 (38%) | Loss: 0.492850\n",
            "Train Epoch: 2 | Batch Status: 23680/60000 (39%) | Loss: 0.376316\n",
            "Train Epoch: 2 | Batch Status: 24320/60000 (41%) | Loss: 0.525611\n",
            "Train Epoch: 2 | Batch Status: 24960/60000 (42%) | Loss: 0.321898\n",
            "Train Epoch: 2 | Batch Status: 25600/60000 (43%) | Loss: 0.459381\n",
            "Train Epoch: 2 | Batch Status: 26240/60000 (44%) | Loss: 0.401230\n",
            "Train Epoch: 2 | Batch Status: 26880/60000 (45%) | Loss: 0.699240\n",
            "Train Epoch: 2 | Batch Status: 27520/60000 (46%) | Loss: 0.327014\n",
            "Train Epoch: 2 | Batch Status: 28160/60000 (47%) | Loss: 0.522768\n",
            "Train Epoch: 2 | Batch Status: 28800/60000 (48%) | Loss: 0.414977\n",
            "Train Epoch: 2 | Batch Status: 29440/60000 (49%) | Loss: 0.597452\n",
            "Train Epoch: 2 | Batch Status: 30080/60000 (50%) | Loss: 0.494604\n",
            "Train Epoch: 2 | Batch Status: 30720/60000 (51%) | Loss: 0.446361\n",
            "Train Epoch: 2 | Batch Status: 31360/60000 (52%) | Loss: 0.590712\n",
            "Train Epoch: 2 | Batch Status: 32000/60000 (53%) | Loss: 0.480630\n",
            "Train Epoch: 2 | Batch Status: 32640/60000 (54%) | Loss: 0.401794\n",
            "Train Epoch: 2 | Batch Status: 33280/60000 (55%) | Loss: 0.465853\n",
            "Train Epoch: 2 | Batch Status: 33920/60000 (57%) | Loss: 0.326100\n",
            "Train Epoch: 2 | Batch Status: 34560/60000 (58%) | Loss: 0.683390\n",
            "Train Epoch: 2 | Batch Status: 35200/60000 (59%) | Loss: 0.525530\n",
            "Train Epoch: 2 | Batch Status: 35840/60000 (60%) | Loss: 0.509268\n",
            "Train Epoch: 2 | Batch Status: 36480/60000 (61%) | Loss: 0.497021\n",
            "Train Epoch: 2 | Batch Status: 37120/60000 (62%) | Loss: 0.641462\n",
            "Train Epoch: 2 | Batch Status: 37760/60000 (63%) | Loss: 0.357390\n",
            "Train Epoch: 2 | Batch Status: 38400/60000 (64%) | Loss: 0.567931\n",
            "Train Epoch: 2 | Batch Status: 39040/60000 (65%) | Loss: 0.337833\n",
            "Train Epoch: 2 | Batch Status: 39680/60000 (66%) | Loss: 0.315504\n",
            "Train Epoch: 2 | Batch Status: 40320/60000 (67%) | Loss: 0.483279\n",
            "Train Epoch: 2 | Batch Status: 40960/60000 (68%) | Loss: 0.297826\n",
            "Train Epoch: 2 | Batch Status: 41600/60000 (69%) | Loss: 0.335043\n",
            "Train Epoch: 2 | Batch Status: 42240/60000 (70%) | Loss: 0.396468\n",
            "Train Epoch: 2 | Batch Status: 42880/60000 (71%) | Loss: 0.334274\n",
            "Train Epoch: 2 | Batch Status: 43520/60000 (72%) | Loss: 0.366002\n",
            "Train Epoch: 2 | Batch Status: 44160/60000 (74%) | Loss: 0.487260\n",
            "Train Epoch: 2 | Batch Status: 44800/60000 (75%) | Loss: 0.712698\n",
            "Train Epoch: 2 | Batch Status: 45440/60000 (76%) | Loss: 0.335303\n",
            "Train Epoch: 2 | Batch Status: 46080/60000 (77%) | Loss: 0.439621\n",
            "Train Epoch: 2 | Batch Status: 46720/60000 (78%) | Loss: 0.422310\n",
            "Train Epoch: 2 | Batch Status: 47360/60000 (79%) | Loss: 0.632582\n",
            "Train Epoch: 2 | Batch Status: 48000/60000 (80%) | Loss: 0.362210\n",
            "Train Epoch: 2 | Batch Status: 48640/60000 (81%) | Loss: 0.290088\n",
            "Train Epoch: 2 | Batch Status: 49280/60000 (82%) | Loss: 0.615345\n",
            "Train Epoch: 2 | Batch Status: 49920/60000 (83%) | Loss: 0.327049\n",
            "Train Epoch: 2 | Batch Status: 50560/60000 (84%) | Loss: 0.428633\n",
            "Train Epoch: 2 | Batch Status: 51200/60000 (85%) | Loss: 0.335307\n",
            "Train Epoch: 2 | Batch Status: 51840/60000 (86%) | Loss: 0.367081\n",
            "Train Epoch: 2 | Batch Status: 52480/60000 (87%) | Loss: 0.666761\n",
            "Train Epoch: 2 | Batch Status: 53120/60000 (88%) | Loss: 0.518344\n",
            "Train Epoch: 2 | Batch Status: 53760/60000 (90%) | Loss: 0.529999\n",
            "Train Epoch: 2 | Batch Status: 54400/60000 (91%) | Loss: 0.364569\n",
            "Train Epoch: 2 | Batch Status: 55040/60000 (92%) | Loss: 0.360228\n",
            "Train Epoch: 2 | Batch Status: 55680/60000 (93%) | Loss: 0.359449\n",
            "Train Epoch: 2 | Batch Status: 56320/60000 (94%) | Loss: 0.581062\n",
            "Train Epoch: 2 | Batch Status: 56960/60000 (95%) | Loss: 0.352855\n",
            "Train Epoch: 2 | Batch Status: 57600/60000 (96%) | Loss: 0.411498\n",
            "Train Epoch: 2 | Batch Status: 58240/60000 (97%) | Loss: 0.585686\n",
            "Train Epoch: 2 | Batch Status: 58880/60000 (98%) | Loss: 0.249659\n",
            "Train Epoch: 2 | Batch Status: 59520/60000 (99%) | Loss: 0.343522\n",
            "Training time: 0m 12s\n",
            "===========================\n",
            "Test set: Average loss: 0.0060, Accuracy: 8878/10000 (89%)\n",
            "Testing time: 0m 14s\n",
            "Train Epoch: 3 | Batch Status: 0/60000 (0%) | Loss: 0.255647\n",
            "Train Epoch: 3 | Batch Status: 640/60000 (1%) | Loss: 0.204666\n",
            "Train Epoch: 3 | Batch Status: 1280/60000 (2%) | Loss: 0.515957\n",
            "Train Epoch: 3 | Batch Status: 1920/60000 (3%) | Loss: 0.455864\n",
            "Train Epoch: 3 | Batch Status: 2560/60000 (4%) | Loss: 0.289843\n",
            "Train Epoch: 3 | Batch Status: 3200/60000 (5%) | Loss: 0.370070\n",
            "Train Epoch: 3 | Batch Status: 3840/60000 (6%) | Loss: 0.466303\n",
            "Train Epoch: 3 | Batch Status: 4480/60000 (7%) | Loss: 0.461464\n",
            "Train Epoch: 3 | Batch Status: 5120/60000 (9%) | Loss: 0.451403\n",
            "Train Epoch: 3 | Batch Status: 5760/60000 (10%) | Loss: 0.243623\n",
            "Train Epoch: 3 | Batch Status: 6400/60000 (11%) | Loss: 0.411845\n",
            "Train Epoch: 3 | Batch Status: 7040/60000 (12%) | Loss: 0.451721\n",
            "Train Epoch: 3 | Batch Status: 7680/60000 (13%) | Loss: 0.590848\n",
            "Train Epoch: 3 | Batch Status: 8320/60000 (14%) | Loss: 0.248374\n",
            "Train Epoch: 3 | Batch Status: 8960/60000 (15%) | Loss: 0.477807\n",
            "Train Epoch: 3 | Batch Status: 9600/60000 (16%) | Loss: 0.194498\n",
            "Train Epoch: 3 | Batch Status: 10240/60000 (17%) | Loss: 0.280601\n",
            "Train Epoch: 3 | Batch Status: 10880/60000 (18%) | Loss: 0.452414\n",
            "Train Epoch: 3 | Batch Status: 11520/60000 (19%) | Loss: 0.520934\n",
            "Train Epoch: 3 | Batch Status: 12160/60000 (20%) | Loss: 0.353390\n",
            "Train Epoch: 3 | Batch Status: 12800/60000 (21%) | Loss: 0.344239\n",
            "Train Epoch: 3 | Batch Status: 13440/60000 (22%) | Loss: 0.319855\n",
            "Train Epoch: 3 | Batch Status: 14080/60000 (23%) | Loss: 0.452025\n",
            "Train Epoch: 3 | Batch Status: 14720/60000 (25%) | Loss: 0.391248\n",
            "Train Epoch: 3 | Batch Status: 15360/60000 (26%) | Loss: 0.393761\n",
            "Train Epoch: 3 | Batch Status: 16000/60000 (27%) | Loss: 0.230765\n",
            "Train Epoch: 3 | Batch Status: 16640/60000 (28%) | Loss: 0.439144\n",
            "Train Epoch: 3 | Batch Status: 17280/60000 (29%) | Loss: 0.219190\n",
            "Train Epoch: 3 | Batch Status: 17920/60000 (30%) | Loss: 0.390116\n",
            "Train Epoch: 3 | Batch Status: 18560/60000 (31%) | Loss: 0.271062\n",
            "Train Epoch: 3 | Batch Status: 19200/60000 (32%) | Loss: 0.236979\n",
            "Train Epoch: 3 | Batch Status: 19840/60000 (33%) | Loss: 0.280070\n",
            "Train Epoch: 3 | Batch Status: 20480/60000 (34%) | Loss: 0.151742\n",
            "Train Epoch: 3 | Batch Status: 21120/60000 (35%) | Loss: 0.354039\n",
            "Train Epoch: 3 | Batch Status: 21760/60000 (36%) | Loss: 0.211083\n",
            "Train Epoch: 3 | Batch Status: 22400/60000 (37%) | Loss: 0.328948\n",
            "Train Epoch: 3 | Batch Status: 23040/60000 (38%) | Loss: 0.312040\n",
            "Train Epoch: 3 | Batch Status: 23680/60000 (39%) | Loss: 0.460895\n",
            "Train Epoch: 3 | Batch Status: 24320/60000 (41%) | Loss: 0.303124\n",
            "Train Epoch: 3 | Batch Status: 24960/60000 (42%) | Loss: 0.271652\n",
            "Train Epoch: 3 | Batch Status: 25600/60000 (43%) | Loss: 0.402528\n",
            "Train Epoch: 3 | Batch Status: 26240/60000 (44%) | Loss: 0.664874\n",
            "Train Epoch: 3 | Batch Status: 26880/60000 (45%) | Loss: 0.268641\n",
            "Train Epoch: 3 | Batch Status: 27520/60000 (46%) | Loss: 0.290611\n",
            "Train Epoch: 3 | Batch Status: 28160/60000 (47%) | Loss: 0.237974\n",
            "Train Epoch: 3 | Batch Status: 28800/60000 (48%) | Loss: 0.226889\n",
            "Train Epoch: 3 | Batch Status: 29440/60000 (49%) | Loss: 0.257440\n",
            "Train Epoch: 3 | Batch Status: 30080/60000 (50%) | Loss: 0.400250\n",
            "Train Epoch: 3 | Batch Status: 30720/60000 (51%) | Loss: 0.164149\n",
            "Train Epoch: 3 | Batch Status: 31360/60000 (52%) | Loss: 0.273963\n",
            "Train Epoch: 3 | Batch Status: 32000/60000 (53%) | Loss: 0.207701\n",
            "Train Epoch: 3 | Batch Status: 32640/60000 (54%) | Loss: 0.305169\n",
            "Train Epoch: 3 | Batch Status: 33280/60000 (55%) | Loss: 0.242733\n",
            "Train Epoch: 3 | Batch Status: 33920/60000 (57%) | Loss: 0.274688\n",
            "Train Epoch: 3 | Batch Status: 34560/60000 (58%) | Loss: 0.186954\n",
            "Train Epoch: 3 | Batch Status: 35200/60000 (59%) | Loss: 0.264123\n",
            "Train Epoch: 3 | Batch Status: 35840/60000 (60%) | Loss: 0.273352\n",
            "Train Epoch: 3 | Batch Status: 36480/60000 (61%) | Loss: 0.334213\n",
            "Train Epoch: 3 | Batch Status: 37120/60000 (62%) | Loss: 0.388967\n",
            "Train Epoch: 3 | Batch Status: 37760/60000 (63%) | Loss: 0.388367\n",
            "Train Epoch: 3 | Batch Status: 38400/60000 (64%) | Loss: 0.687836\n",
            "Train Epoch: 3 | Batch Status: 39040/60000 (65%) | Loss: 0.278064\n",
            "Train Epoch: 3 | Batch Status: 39680/60000 (66%) | Loss: 0.199085\n",
            "Train Epoch: 3 | Batch Status: 40320/60000 (67%) | Loss: 0.204166\n",
            "Train Epoch: 3 | Batch Status: 40960/60000 (68%) | Loss: 0.286561\n",
            "Train Epoch: 3 | Batch Status: 41600/60000 (69%) | Loss: 0.163176\n",
            "Train Epoch: 3 | Batch Status: 42240/60000 (70%) | Loss: 0.382435\n",
            "Train Epoch: 3 | Batch Status: 42880/60000 (71%) | Loss: 0.448229\n",
            "Train Epoch: 3 | Batch Status: 43520/60000 (72%) | Loss: 0.263935\n",
            "Train Epoch: 3 | Batch Status: 44160/60000 (74%) | Loss: 0.361263\n",
            "Train Epoch: 3 | Batch Status: 44800/60000 (75%) | Loss: 0.184469\n",
            "Train Epoch: 3 | Batch Status: 45440/60000 (76%) | Loss: 0.260298\n",
            "Train Epoch: 3 | Batch Status: 46080/60000 (77%) | Loss: 0.230602\n",
            "Train Epoch: 3 | Batch Status: 46720/60000 (78%) | Loss: 0.297966\n",
            "Train Epoch: 3 | Batch Status: 47360/60000 (79%) | Loss: 0.167306\n",
            "Train Epoch: 3 | Batch Status: 48000/60000 (80%) | Loss: 0.352277\n",
            "Train Epoch: 3 | Batch Status: 48640/60000 (81%) | Loss: 0.356847\n",
            "Train Epoch: 3 | Batch Status: 49280/60000 (82%) | Loss: 0.338136\n",
            "Train Epoch: 3 | Batch Status: 49920/60000 (83%) | Loss: 0.379370\n",
            "Train Epoch: 3 | Batch Status: 50560/60000 (84%) | Loss: 0.392049\n",
            "Train Epoch: 3 | Batch Status: 51200/60000 (85%) | Loss: 0.277022\n",
            "Train Epoch: 3 | Batch Status: 51840/60000 (86%) | Loss: 0.376085\n",
            "Train Epoch: 3 | Batch Status: 52480/60000 (87%) | Loss: 0.281187\n",
            "Train Epoch: 3 | Batch Status: 53120/60000 (88%) | Loss: 0.132034\n",
            "Train Epoch: 3 | Batch Status: 53760/60000 (90%) | Loss: 0.317529\n",
            "Train Epoch: 3 | Batch Status: 54400/60000 (91%) | Loss: 0.154504\n",
            "Train Epoch: 3 | Batch Status: 55040/60000 (92%) | Loss: 0.163578\n",
            "Train Epoch: 3 | Batch Status: 55680/60000 (93%) | Loss: 0.177480\n",
            "Train Epoch: 3 | Batch Status: 56320/60000 (94%) | Loss: 0.326668\n",
            "Train Epoch: 3 | Batch Status: 56960/60000 (95%) | Loss: 0.320384\n",
            "Train Epoch: 3 | Batch Status: 57600/60000 (96%) | Loss: 0.169108\n",
            "Train Epoch: 3 | Batch Status: 58240/60000 (97%) | Loss: 0.565819\n",
            "Train Epoch: 3 | Batch Status: 58880/60000 (98%) | Loss: 0.315422\n",
            "Train Epoch: 3 | Batch Status: 59520/60000 (99%) | Loss: 0.215424\n",
            "Training time: 0m 12s\n",
            "===========================\n",
            "Test set: Average loss: 0.0043, Accuracy: 9206/10000 (92%)\n",
            "Testing time: 0m 14s\n",
            "Train Epoch: 4 | Batch Status: 0/60000 (0%) | Loss: 0.207564\n",
            "Train Epoch: 4 | Batch Status: 640/60000 (1%) | Loss: 0.418382\n",
            "Train Epoch: 4 | Batch Status: 1280/60000 (2%) | Loss: 0.281406\n",
            "Train Epoch: 4 | Batch Status: 1920/60000 (3%) | Loss: 0.133949\n",
            "Train Epoch: 4 | Batch Status: 2560/60000 (4%) | Loss: 0.472163\n",
            "Train Epoch: 4 | Batch Status: 3200/60000 (5%) | Loss: 0.221252\n",
            "Train Epoch: 4 | Batch Status: 3840/60000 (6%) | Loss: 0.164899\n",
            "Train Epoch: 4 | Batch Status: 4480/60000 (7%) | Loss: 0.411126\n",
            "Train Epoch: 4 | Batch Status: 5120/60000 (9%) | Loss: 0.295269\n",
            "Train Epoch: 4 | Batch Status: 5760/60000 (10%) | Loss: 0.196095\n",
            "Train Epoch: 4 | Batch Status: 6400/60000 (11%) | Loss: 0.137985\n",
            "Train Epoch: 4 | Batch Status: 7040/60000 (12%) | Loss: 0.311254\n",
            "Train Epoch: 4 | Batch Status: 7680/60000 (13%) | Loss: 0.460078\n",
            "Train Epoch: 4 | Batch Status: 8320/60000 (14%) | Loss: 0.145569\n",
            "Train Epoch: 4 | Batch Status: 8960/60000 (15%) | Loss: 0.270114\n",
            "Train Epoch: 4 | Batch Status: 9600/60000 (16%) | Loss: 0.414210\n",
            "Train Epoch: 4 | Batch Status: 10240/60000 (17%) | Loss: 0.200208\n",
            "Train Epoch: 4 | Batch Status: 10880/60000 (18%) | Loss: 0.240265\n",
            "Train Epoch: 4 | Batch Status: 11520/60000 (19%) | Loss: 0.505555\n",
            "Train Epoch: 4 | Batch Status: 12160/60000 (20%) | Loss: 0.279676\n",
            "Train Epoch: 4 | Batch Status: 12800/60000 (21%) | Loss: 0.219084\n",
            "Train Epoch: 4 | Batch Status: 13440/60000 (22%) | Loss: 0.139305\n",
            "Train Epoch: 4 | Batch Status: 14080/60000 (23%) | Loss: 0.230348\n",
            "Train Epoch: 4 | Batch Status: 14720/60000 (25%) | Loss: 0.268192\n",
            "Train Epoch: 4 | Batch Status: 15360/60000 (26%) | Loss: 0.079077\n",
            "Train Epoch: 4 | Batch Status: 16000/60000 (27%) | Loss: 0.298953\n",
            "Train Epoch: 4 | Batch Status: 16640/60000 (28%) | Loss: 0.212628\n",
            "Train Epoch: 4 | Batch Status: 17280/60000 (29%) | Loss: 0.424824\n",
            "Train Epoch: 4 | Batch Status: 17920/60000 (30%) | Loss: 0.185432\n",
            "Train Epoch: 4 | Batch Status: 18560/60000 (31%) | Loss: 0.342387\n",
            "Train Epoch: 4 | Batch Status: 19200/60000 (32%) | Loss: 0.377133\n",
            "Train Epoch: 4 | Batch Status: 19840/60000 (33%) | Loss: 0.106988\n",
            "Train Epoch: 4 | Batch Status: 20480/60000 (34%) | Loss: 0.341834\n",
            "Train Epoch: 4 | Batch Status: 21120/60000 (35%) | Loss: 0.171467\n",
            "Train Epoch: 4 | Batch Status: 21760/60000 (36%) | Loss: 0.303443\n",
            "Train Epoch: 4 | Batch Status: 22400/60000 (37%) | Loss: 0.314645\n",
            "Train Epoch: 4 | Batch Status: 23040/60000 (38%) | Loss: 0.136491\n",
            "Train Epoch: 4 | Batch Status: 23680/60000 (39%) | Loss: 0.354226\n",
            "Train Epoch: 4 | Batch Status: 24320/60000 (41%) | Loss: 0.165996\n",
            "Train Epoch: 4 | Batch Status: 24960/60000 (42%) | Loss: 0.417661\n",
            "Train Epoch: 4 | Batch Status: 25600/60000 (43%) | Loss: 0.201529\n",
            "Train Epoch: 4 | Batch Status: 26240/60000 (44%) | Loss: 0.169227\n",
            "Train Epoch: 4 | Batch Status: 26880/60000 (45%) | Loss: 0.209458\n",
            "Train Epoch: 4 | Batch Status: 27520/60000 (46%) | Loss: 0.123814\n",
            "Train Epoch: 4 | Batch Status: 28160/60000 (47%) | Loss: 0.164639\n",
            "Train Epoch: 4 | Batch Status: 28800/60000 (48%) | Loss: 0.197607\n",
            "Train Epoch: 4 | Batch Status: 29440/60000 (49%) | Loss: 0.152671\n",
            "Train Epoch: 4 | Batch Status: 30080/60000 (50%) | Loss: 0.370743\n",
            "Train Epoch: 4 | Batch Status: 30720/60000 (51%) | Loss: 0.261335\n",
            "Train Epoch: 4 | Batch Status: 31360/60000 (52%) | Loss: 0.232094\n",
            "Train Epoch: 4 | Batch Status: 32000/60000 (53%) | Loss: 0.177949\n",
            "Train Epoch: 4 | Batch Status: 32640/60000 (54%) | Loss: 0.429574\n",
            "Train Epoch: 4 | Batch Status: 33280/60000 (55%) | Loss: 0.148664\n",
            "Train Epoch: 4 | Batch Status: 33920/60000 (57%) | Loss: 0.266350\n",
            "Train Epoch: 4 | Batch Status: 34560/60000 (58%) | Loss: 0.272021\n",
            "Train Epoch: 4 | Batch Status: 35200/60000 (59%) | Loss: 0.363636\n",
            "Train Epoch: 4 | Batch Status: 35840/60000 (60%) | Loss: 0.171310\n",
            "Train Epoch: 4 | Batch Status: 36480/60000 (61%) | Loss: 0.266411\n",
            "Train Epoch: 4 | Batch Status: 37120/60000 (62%) | Loss: 0.225912\n",
            "Train Epoch: 4 | Batch Status: 37760/60000 (63%) | Loss: 0.304275\n",
            "Train Epoch: 4 | Batch Status: 38400/60000 (64%) | Loss: 0.129721\n",
            "Train Epoch: 4 | Batch Status: 39040/60000 (65%) | Loss: 0.219678\n",
            "Train Epoch: 4 | Batch Status: 39680/60000 (66%) | Loss: 0.388993\n",
            "Train Epoch: 4 | Batch Status: 40320/60000 (67%) | Loss: 0.217185\n",
            "Train Epoch: 4 | Batch Status: 40960/60000 (68%) | Loss: 0.262945\n",
            "Train Epoch: 4 | Batch Status: 41600/60000 (69%) | Loss: 0.189434\n",
            "Train Epoch: 4 | Batch Status: 42240/60000 (70%) | Loss: 0.293700\n",
            "Train Epoch: 4 | Batch Status: 42880/60000 (71%) | Loss: 0.132901\n",
            "Train Epoch: 4 | Batch Status: 43520/60000 (72%) | Loss: 0.267044\n",
            "Train Epoch: 4 | Batch Status: 44160/60000 (74%) | Loss: 0.140926\n",
            "Train Epoch: 4 | Batch Status: 44800/60000 (75%) | Loss: 0.150773\n",
            "Train Epoch: 4 | Batch Status: 45440/60000 (76%) | Loss: 0.370828\n",
            "Train Epoch: 4 | Batch Status: 46080/60000 (77%) | Loss: 0.437016\n",
            "Train Epoch: 4 | Batch Status: 46720/60000 (78%) | Loss: 0.211375\n",
            "Train Epoch: 4 | Batch Status: 47360/60000 (79%) | Loss: 0.163025\n",
            "Train Epoch: 4 | Batch Status: 48000/60000 (80%) | Loss: 0.278220\n",
            "Train Epoch: 4 | Batch Status: 48640/60000 (81%) | Loss: 0.221210\n",
            "Train Epoch: 4 | Batch Status: 49280/60000 (82%) | Loss: 0.171884\n",
            "Train Epoch: 4 | Batch Status: 49920/60000 (83%) | Loss: 0.412950\n",
            "Train Epoch: 4 | Batch Status: 50560/60000 (84%) | Loss: 0.228391\n",
            "Train Epoch: 4 | Batch Status: 51200/60000 (85%) | Loss: 0.100215\n",
            "Train Epoch: 4 | Batch Status: 51840/60000 (86%) | Loss: 0.119666\n",
            "Train Epoch: 4 | Batch Status: 52480/60000 (87%) | Loss: 0.064228\n",
            "Train Epoch: 4 | Batch Status: 53120/60000 (88%) | Loss: 0.204459\n",
            "Train Epoch: 4 | Batch Status: 53760/60000 (90%) | Loss: 0.126568\n",
            "Train Epoch: 4 | Batch Status: 54400/60000 (91%) | Loss: 0.302018\n",
            "Train Epoch: 4 | Batch Status: 55040/60000 (92%) | Loss: 0.195366\n",
            "Train Epoch: 4 | Batch Status: 55680/60000 (93%) | Loss: 0.232596\n",
            "Train Epoch: 4 | Batch Status: 56320/60000 (94%) | Loss: 0.119254\n",
            "Train Epoch: 4 | Batch Status: 56960/60000 (95%) | Loss: 0.235558\n",
            "Train Epoch: 4 | Batch Status: 57600/60000 (96%) | Loss: 0.188588\n",
            "Train Epoch: 4 | Batch Status: 58240/60000 (97%) | Loss: 0.088922\n",
            "Train Epoch: 4 | Batch Status: 58880/60000 (98%) | Loss: 0.177358\n",
            "Train Epoch: 4 | Batch Status: 59520/60000 (99%) | Loss: 0.269120\n",
            "Training time: 0m 12s\n",
            "===========================\n",
            "Test set: Average loss: 0.0032, Accuracy: 9371/10000 (94%)\n",
            "Testing time: 0m 14s\n",
            "Total Time: 0m 54s\n",
            "Model was trained on cpu!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2GLiVsoZA_O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}